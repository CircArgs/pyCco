{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0204f5e7-92dc-405d-bfae-b2dc68464d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycco.tokenizers import *\n",
    "from pycco.ast import *\n",
    "from pycco.parsers import *\n",
    "from inspect import signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "668b0e09-f1a7-487a-a8d6-efc4e4c1dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Uses Parsers to transform a token stream into an AST\"\"\"\n",
    "from typing import List, Optional\n",
    "from pycco.parser import match, Parser, parser, S, T, U, ParserResult\n",
    "from pycco.tokens import Token, TokenKind\n",
    "from pycco import ast\n",
    "\n",
    "# Define token matchers\n",
    "type = match(TokenKind.TYPE)\n",
    "name = match(TokenKind.IDENTIFIER)\n",
    "open_paren = match(Token(TokenKind.SYMBOL, \"(\"))\n",
    "close_paren = match(Token(TokenKind.SYMBOL, \")\"))\n",
    "open_bracket = match(Token(TokenKind.SYMBOL, \"{\"))\n",
    "close_bracket = match(Token(TokenKind.SYMBOL, \"}\"))\n",
    "comma = match(Token(TokenKind.SYMBOL, \",\"))\n",
    "semicolon = match(Token(TokenKind.SYMBOL, \";\"))\n",
    "eq = match(Token(TokenKind.OPERATOR, \"=\"))\n",
    "star = match(Token(TokenKind.OPERATOR, \"*\"))\n",
    "\n",
    "# Grammar Components with Map Functions\n",
    "\n",
    "# Variable Declaration\n",
    "def map_variable_decl(tokens: List[Token]) -> ast.VariableDecl:\n",
    "    \"\"\"\n",
    "    Map tokens to a VariableDecl.\n",
    "    Assumes tokens are in the order: type [*] identifier ;\n",
    "    \"\"\"\n",
    "    type = ast.Type(tokens[0].value).set_tokens([tokens[0]])\n",
    "    pointer = semicolon = False\n",
    "    if len(tokens) > 2:\n",
    "        pointer = tokens[1].value == \"*\"\n",
    "        semicolon = tokens[-1].value == ';'\n",
    "    \n",
    "    name_token = next(token for token in tokens[1:] if token.kind == TokenKind.IDENTIFIER)\n",
    "    name = ast.Ident(name_token.value).set_tokens([name_token])\n",
    "    type.pointer = pointer\n",
    "    return ast.VariableDecl(type=type, name=name, semicolon=semicolon).set_tokens(tokens)\n",
    "\n",
    "\n",
    "def map_number(token: Token)->ast.Number:\n",
    "    type = ast.Type('int')\n",
    "    if '.' in token.value:\n",
    "        type = ast.Type('float')\n",
    "    return ast.Number(token.value, type).set_tokens([token])\n",
    "    \n",
    "number = match(Token(TokenKind.NUMBER)) @ map_number\n",
    "\n",
    "def map_string(token: Token)->ast.StringLiteral:\n",
    "    return ast.StringLiteral(token.value).set_tokens([token])\n",
    "    \n",
    "string = match(Token(TokenKind.STRING_LITERAL)) @ map_string\n",
    "\n",
    "variable_ = type + ~star + name\n",
    "variable = variable_ @ map_variable_decl\n",
    "# type with optional * with an identifier and optional ;\n",
    "variable_decl = (variable_ + semicolon) @ map_variable_decl\n",
    "\n",
    "args = open_paren >> variable.sep_by(Token(TokenKind.SYMBOL, ',')) << close_paren\n",
    "\n",
    "@parser\n",
    "def statement(stream: List[Token], index: int)-> ParserResult[ast.Statement]:\n",
    "    return (return_ | variable_decl).parse_fn(stream, index) \n",
    "\n",
    "@parser\n",
    "def expression(stream: List[Token], index: int)-> ParserResult[ast.Expression]:\n",
    "    return (number | string | expression).parse_fn(stream, index) \n",
    "\n",
    "def map_return(expr: ast.Expression)->ast.Return:\n",
    "    return ast.Return(expr)\n",
    "\n",
    "return_ = match(Token(TokenKind.KEYWORD, 'return')) >> expression << semicolon\n",
    "return_ @= map_return\n",
    "\n",
    "\n",
    "# need to declare in sta\n",
    "function = variable + args\n",
    "function += open_bracket >> statement.until(close_bracket) << close_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d20d4a-5fb0-4e9f-ae08-c54612bbaf23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number(value='2', type=Type(name='int', pointer=False))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expression(tokenize('2 ')).result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82438451-c9b1-4c9c-8f66-b9a5d67ed4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parser(\"}\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "993e988e-157a-4f0c-8b15-8b4891c3fe69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(kind=<TokenKind.KEYWORD: 2>, value='return', start=6, end=11),\n",
       " Token(kind=<TokenKind.NUMBER: 7>, value='2', start=8, end=8),\n",
       " Token(kind=<TokenKind.SYMBOL: 4>, value=';', start=9, end=9),\n",
       " Token(kind=<TokenKind.SYMBOL: 4>, value='}', start=10, end=10),\n",
       " Token(kind=<TokenKind.EOF: 10>, value=None, start=10, end=10)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('return 2;}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc0ec07a-c0a5-4adf-a986-7f8c2cd757fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Return(value=[Token(kind=<TokenKind.KEYWORD: 2>, value='return', start=6, end=11), Token(kind=<TokenKind.NUMBER: 7>, value='2', start=8, end=8), Token(kind=<TokenKind.SYMBOL: 4>, value=';', start=9, end=9)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(statement.until(close_bracket)@map_return)(tokenize('return 2;}')).result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70f1e431-93bd-4ad9-9e18-8bebd02df5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParserResult(index=3, result=return 2;, description=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement(tokenize('return 2;'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da3bc27a-cd2c-4fe2-9f8f-700127ce6391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(kind=<TokenKind.KEYWORD: 2>, value='return', start=13, end=18),\n",
       " Token(kind=<TokenKind.NUMBER: 7>, value='2', start=15, end=15),\n",
       " Token(kind=<TokenKind.SYMBOL: 4>, value=';', start=16, end=16)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(open_bracket >> statement.until(close_bracket) << close_bracket)(tokenize('''\n",
    "{\n",
    "    return 2;\n",
    "}''')).result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66e903-6e00-423c-ace0-2ac6881a1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Expression\n",
    "def map_binary_expression(tokens: List[Token]) -> ast.BinaryExpression:\n",
    "    \"\"\"\n",
    "    Map tokens to a BinaryExpression.\n",
    "    Assumes tokens are in the order: left operator right\n",
    "    \"\"\"\n",
    "    left = tokens[0]\n",
    "    operator = tokens[1].value\n",
    "    right = tokens[2]\n",
    "    return ast.BinaryExpression(\n",
    "        left=ast.Ident(left.value) if left.kind == TokenKind.IDENTIFIER else left,\n",
    "        operator=operator,\n",
    "        right=ast.Ident(right.value) if right.kind == TokenKind.IDENTIFIER else right,\n",
    "    )\n",
    "\n",
    "expression = Parser()  # Forward declaration for expressions\n",
    "binary_expression = (expression + TokenKind.OPERATOR + expression) @ map_binary_expression\n",
    "\n",
    "# If Statement\n",
    "def map_if_statement(tokens: List[Token]) -> ast.IfStatement:\n",
    "    \"\"\"\n",
    "    Map tokens to an IfStatement.\n",
    "    Assumes tokens are in the order: if (condition) { then_branch } [else { else_branch }]\n",
    "    \"\"\"\n",
    "    condition = tokens[0]  # First token after `if` and `(`\n",
    "    then_branch = tokens[1]  # Tokens within the `{}` following the condition\n",
    "    else_branch = tokens[2] if len(tokens) > 2 else None  # Optional `else` branch\n",
    "    return ast.IfStatement(condition=condition, then_branch=then_branch, else_branch=else_branch)\n",
    "\n",
    "if_statement = (\n",
    "    match(Token(TokenKind.KEYWORD, \"if\"))\n",
    "    >> open_paren\n",
    "    >> expression\n",
    "    << close_paren\n",
    "    + open_bracket\n",
    "    >> variable_decl.until(close_bracket)\n",
    "    << close_bracket\n",
    "    + (\n",
    "        match(Token(TokenKind.KEYWORD, \"else\"))\n",
    "        >> open_bracket\n",
    "        >> variable_decl.until(close_bracket)\n",
    "        << close_bracket\n",
    "    ).optional()\n",
    ") @ map_if_statement\n",
    "\n",
    "# While Loop\n",
    "def map_while_loop(tokens: List[Token]) -> ast.WhileLoop:\n",
    "    \"\"\"\n",
    "    Map tokens to a WhileLoop.\n",
    "    Assumes tokens are in the order: while (condition) { body }\n",
    "    \"\"\"\n",
    "    condition = tokens[0]  # First token after `while` and `(`\n",
    "    body = tokens[1]  # Tokens within the `{}` following the condition\n",
    "    return ast.WhileLoop(condition=condition, body=body)\n",
    "\n",
    "while_loop = (\n",
    "    match(Token(TokenKind.KEYWORD, \"while\"))\n",
    "    >> open_paren\n",
    "    >> expression\n",
    "    << close_paren\n",
    "    + open_bracket\n",
    "    >> variable_decl.until(close_bracket)\n",
    "    << close_bracket\n",
    ") @ map_while_loop\n",
    "\n",
    "# Function\n",
    "def map_function(tokens: List[Token]) -> ast.Function:\n",
    "    \"\"\"\n",
    "    Map tokens to a Function.\n",
    "    Assumes tokens are in the order: return_type name(args) { body }\n",
    "    \"\"\"\n",
    "    return_type = ast.Type(tokens[0].value)\n",
    "    name = ast.Ident(tokens[1].value)\n",
    "    args = [map_variable_decl(arg) for arg in tokens[2]]  # Tokens within `()`\n",
    "    body = tokens[3]  # Tokens within `{}`\n",
    "    return ast.Function(\n",
    "        var=ast.VariableDecl(type=return_type, name=name),\n",
    "        args=args,\n",
    "        body=body,\n",
    "    )\n",
    "\n",
    "# Forward declaration for statement\n",
    "statement = Parser()\n",
    "\n",
    "# Function parser\n",
    "def map_function(tokens: List[Token]) -> ast.Function:\n",
    "    \"\"\"\n",
    "    Map tokens to a Function.\n",
    "    Assumes tokens are in the order: return_type name(args) { body }\n",
    "    \"\"\"\n",
    "    return_type = ast.Type(tokens[0].value)\n",
    "    name = ast.Ident(tokens[1].value)\n",
    "    args = [map_variable_decl(arg) for arg in tokens[2]]  # Tokens within `()`\n",
    "    body = tokens[3]  # Tokens within `{}`\n",
    "    return ast.Function(\n",
    "        var=ast.VariableDecl(type=return_type, name=name),\n",
    "        args=args,\n",
    "        body=body,\n",
    "    )\n",
    "\n",
    "function = (\n",
    "    type\n",
    "    + name\n",
    "    + (open_paren >> variable.until(comma.optional() + close_paren))\n",
    "    << close_paren\n",
    "    + open_bracket\n",
    "    >> statement.until(close_bracket)\n",
    "    << close_bracket\n",
    ") @ map_function\n",
    "\n",
    "# Define the statement parser\n",
    "statement.define(variable_decl | if_statement | while_loop | binary_expression)\n",
    "\n",
    "\n",
    "# Update the `_expression` placeholder to include binary_expression\n",
    "expression.define(binary_expression | variable)\n",
    "\n",
    "\n",
    "# Entry Point for Parsing a Program\n",
    "program = function.until(TokenKind.EOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b90ee94-47bb-42a8-ade4-a40568bff719",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code = \"\"\"\n",
    "int* main(args) {\n",
    "    int* x = 42;\n",
    "        /*\n",
    "    multiline comment\n",
    "    */\n",
    "    printf(\"Hello, World!\");\n",
    "    return 0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a4b6bdd-7baf-4489-9d91-fd3c896b2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(source_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61404ddc-4c62-4b86-8762-620064e38a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = type + star.optional() + name\n",
    "variable_decl = (variable + semicolon.optional()) @ map_variable_decl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17e57e29-49c7-4a63-b526-75eaa073c14f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvariable_decl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint main\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/pyCco/pycco/parser.py:111\u001b[0m, in \u001b[0;36mParser.__call__\u001b[0;34m(self, stream, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03mInvoke the parser on the given stream starting at the given index.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    ParserResult[T]: A tuple of the next index and the parsed result (or None on failure).\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m description \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_fn, Parser)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_fn\u001b[38;5;241m.\u001b[39mdescription\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    110\u001b[0m index, result \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_fn, Parser)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_fn\u001b[38;5;241m.\u001b[39mparse_fn\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ParserResult(index, result, description)\n",
      "File \u001b[0;32m~/Projects/pyCco/pycco/parser.py:162\u001b[0m, in \u001b[0;36mParser.map.<locals>.parse\u001b[0;34m(stream, index)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ParserResult()\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(signature(mapper)\u001b[38;5;241m.\u001b[39mparameters)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m \u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m mapper(res, i)\n",
      "File \u001b[0;32m~/Projects/pyCco/pycco/parsers.py:31\u001b[0m, in \u001b[0;36mmap_variable_decl\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     29\u001b[0m     pointer \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m     semicolon \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 31\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTokenKind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIDENTIFIER\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n\u001b[1;32m     32\u001b[0m name \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mIdent(name)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m.\u001b[39mpointer \u001b[38;5;241m=\u001b[39m pointer\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "variable_decl(tokenize('int main'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eae1f7e-ce70-459c-a605-41a66a117c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParserResult(index=2, result=[Token(kind=<TokenKind.TYPE: 3>, value='int', start=3, end=5), Token(kind=<TokenKind.IDENTIFIER: 1>, value='main', start=8, end=11)], description=(((\"TYPE\" + \"*\"?) + \"IDENTIFIER\") + \";\"?))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_decl(tokenize('int main'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dcc8707-b2da-4299-a451-1c777680f093",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "Parsing failed due to an unexpected error.\nAt token index 0:\nint main ( args )\n^\nToken causing issue: int (kind: TYPE)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Projects/pyCco/pycco/parsers.py:216\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Check if parsing consumed all tokens\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mparser_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_index\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParsing did not consume all tokens. Remaining tokens detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         tokens,\n\u001b[1;32m    220\u001b[0m         parser_result\u001b[38;5;241m.\u001b[39mnext_index,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ParserResult' object has no attribute 'next_index'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mParseError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/pyCco/pycco/parsers.py:230\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e  \u001b[38;5;66;03m# Re-raise existing ParseError\u001b[39;00m\n\u001b[1;32m    229\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Extract index if available\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParsing failed due to an unexpected error.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    232\u001b[0m     tokens,\n\u001b[1;32m    233\u001b[0m     index \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    234\u001b[0m ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mParseError\u001b[0m: Parsing failed due to an unexpected error.\nAt token index 0:\nint main ( args )\n^\nToken causing issue: int (kind: TYPE)"
     ]
    }
   ],
   "source": [
    "parse(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ede8cd-0cc6-4847-bbe7-26ed4551b98b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variable_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r\u001b[38;5;241m=\u001b[39m(variable\u001b[38;5;129m@variable_map\u001b[39m)(tokenize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint *main\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'variable_map' is not defined"
     ]
    }
   ],
   "source": [
    "r=(variable@variable_map)(tokenize('int *main'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e9e46b-77d5-45ff-8edf-b0a61ec4f973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(kind=<TokenKind.TYPE: 3>, value='int', start=3, end=5),\n",
       " Token(kind=<TokenKind.OPERATOR: 5>, value='*', start=5, end=5),\n",
       " Token(kind=<TokenKind.IDENTIFIER: 1>, value='main', start=9, end=12)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.result.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0bf5f2-5cd9-47a6-a1bc-ebce376d0f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParserResult(index=-1, result=None, description=(\"TYPE\" + \"IDENTIFIER\"))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(type + name)(tokenizer(('int main')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88668f5a-6940-410c-acc6-0062b20ca566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParserResult(index=-1, result=None, description=((\"TYPE\" + \"IDENTIFIER\") + (\"(\" >> (((\"TYPE\" + \"*\"?) + \"IDENTIFIER\") until (\",\"? + \")\")))))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(type + name + (open_paren >> variable.until(comma.optional() + close_paren)))(source_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8921f400-b83c-4528-a9e7-0991e184a679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParserResult(index=1, result=int, description=\"TYPE\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenize('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13c4cd9-2c39-4c81-abc4-3f71bdf30028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParserResult(index=4, result=[Token(kind=<TokenKind.TYPE: 3>, value='int', start=3, end=5), Token(kind=<TokenKind.OPERATOR: 5>, value='*', start=4, end=4), Token(kind=<TokenKind.IDENTIFIER: 1>, value='x', start=6, end=6), Token(kind=<TokenKind.SYMBOL: 4>, value=';', start=7, end=7)], description=(((\"TYPE\" + \"*\"?) + \"IDENTIFIER\") + \";\"))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_decl(tokenize('int* x;'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a03581-2d7a-4833-802b-ca0b0a675252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParserResult(index=21, result=[Token(kind=<TokenKind.TYPE: 3>, value='int', start=25, end=27), Token(kind=<TokenKind.OPERATOR: 5>, value='*', start=26, end=26), Token(kind=<TokenKind.IDENTIFIER: 1>, value='x', start=28, end=28), Token(kind=<TokenKind.OPERATOR: 5>, value='=', start=30, end=30), Token(kind=<TokenKind.NUMBER: 7>, value='42', start=33, end=34), Token(kind=<TokenKind.SYMBOL: 4>, value=';', start=34, end=34), Token(kind=<TokenKind.IDENTIFIER: 1>, value='printf', start=85, end=90), Token(kind=<TokenKind.SYMBOL: 4>, value='(', start=86, end=86), Token(kind=<TokenKind.STRING_LITERAL: 6>, value='Hello, World!', start=101, end=113), Token(kind=<TokenKind.SYMBOL: 4>, value=')', start=102, end=102), Token(kind=<TokenKind.SYMBOL: 4>, value=';', start=103, end=103), Token(kind=<TokenKind.KEYWORD: 2>, value='return', start=114, end=119), Token(kind=<TokenKind.NUMBER: 7>, value='0', start=116, end=116), Token(kind=<TokenKind.SYMBOL: 4>, value=';', start=117, end=117)], description=(((((\"TYPE\" + \"IDENTIFIER\") + (\"(\" >> (((\"TYPE\" + \"*\"?) + \"IDENTIFIER\") until (\",\"? + \")\")))) << (\")\" + \"{\")) >> (. until \"}\")) << \"}\"))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function(tokenize(source_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44275a4-4f75-41b5-9691-8c627d7ef872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(kind=<TokenKind.TYPE: 3>, value='int', start=4, end=6),\n",
       " Token(kind=<TokenKind.IDENTIFIER: 1>, value='main', start=9, end=12),\n",
       " Token(kind=<TokenKind.SYMBOL: 4>, value='(', start=10, end=10),\n",
       " Token(kind=<TokenKind.IDENTIFIER: 1>, value='args', start=14, end=17),\n",
       " Token(kind=<TokenKind.SYMBOL: 4>, value=')', start=15, end=15),\n",
       " Token(kind=<TokenKind.SYMBOL: 4>, value='{', start=17, end=17),\n",
       " Token(kind=<TokenKind.TYPE: 3>, value='int', start=25, end=27),\n",
       " Token(kind=<TokenKind.OPERATOR: 5>, value='*', start=26, end=26),\n",
       " Token(kind=<TokenKind.IDENTIFIER: 1>, value='x', start=28, end=28),\n",
       " Token(kind=<TokenKind.OPERATOR: 5>, value='=', start=30, end=30),\n",
       " Token(kind=<TokenKind.NUMBER: 7>, value='42', start=33, end=34),\n",
       " Token(kind=<TokenKind.SYMBOL: 4>, value=';', start=34, end=34),\n",
       " Token(kind=<TokenKind.IDENTIFIER: 1>, value='printf', start=85, end=90),\n",
       " Token(kind=<TokenKind.SYMBOL: 4>, value='(', start=86, end=86),\n",
       " Token(kind=<TokenKind.STRING_LITERAL: 6>, value='Hello, World!', start=101, end=113),\n",
       " Token(kind=<TokenKind.SYMBOL: 4>, value=')', start=102, end=102),\n",
       " Token(kind=<TokenKind.SYMBOL: 4>, value=';', start=103, end=103),\n",
       " Token(kind=<TokenKind.KEYWORD: 2>, value='return', start=114, end=119),\n",
       " Token(kind=<TokenKind.NUMBER: 7>, value='0', start=116, end=116),\n",
       " Token(kind=<TokenKind.SYMBOL: 4>, value=';', start=117, end=117),\n",
       " Token(kind=<TokenKind.SYMBOL: 4>, value='}', start=119, end=119)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(source_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d063a87-ae19-43b2-8554-c95a06cac812",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_code = \"\"\"\n",
    "int main() {\n",
    "    int x = 42;\n",
    "    /*\n",
    "    multiline comment\n",
    "    */\n",
    "    printf(Hello, World!\");\n",
    "    return 0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9289690d-cc80-4cba-b456-1423b8091781",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tokenization Error:\nUnexpected token at line 7, column 25:\n\n    multiline comment\n    */\n    printf(Hello, World!\");\n                        ^\n    return 0;\n}\nExpected one of whitespace, comment, keywords, keywords, (one of {, }, (, ), ;, ,, [, ], ., -> @ create_token), (one of +, -, *, /, %, =, ==, !=, <, >, <=, >=, &&, ||, !, &, |, ^, <<, >> @ create_token), number, string literal, identifier.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbad_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/pyCco/pycco/tokenizers.py:167\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize\u001b[39m(source: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Token]:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miter_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/pyCco/pycco/tokenizers.py:150\u001b[0m, in \u001b[0;36miter_tokenize\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m    147\u001b[0m     pointer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m column_number \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Raise detailed error\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenization Error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected token at line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline_number\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_number\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbefore_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_line\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpointer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mafter_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (TokenKind\u001b[38;5;241m.\u001b[39mWHITESPACE, TokenKind\u001b[38;5;241m.\u001b[39mCOMMENT):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m token\n",
      "\u001b[0;31mValueError\u001b[0m: Tokenization Error:\nUnexpected token at line 7, column 25:\n\n    multiline comment\n    */\n    printf(Hello, World!\");\n                        ^\n    return 0;\n}\nExpected one of whitespace, comment, keywords, keywords, (one of {, }, (, ), ;, ,, [, ], ., -> @ create_token), (one of +, -, *, /, %, =, ==, !=, <, >, <=, >=, &&, ||, !, &, |, ^, <<, >> @ create_token), number, string literal, identifier.\n"
     ]
    }
   ],
   "source": [
    "tokenize(bad_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20db9086-5644-45b2-aa20-b5dbac88216d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parser(one of whitespace, comment, keywords, keywords, (one of {, }, (, ), ;, ,, [, ], ., -> @ create_token), (one of +, -, *, /, %, =, ==, !=, <, >, <=, >=, &&, ||, !, &, |, ^, <<, >> @ create_token), number, string literal, identifier)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330a2e2-8f1c-4491-8239-2fd7a2445851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
